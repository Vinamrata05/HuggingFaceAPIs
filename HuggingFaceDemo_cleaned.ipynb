{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinamrata05/HuggingFaceAPIs/blob/main/HuggingFaceDemo_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VsOdRmXiFRe"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Dbpvb0EonJ",
        "outputId": "38ae7684-d7e6-4147-9bc4-3f1ab484a1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jun 10 21:58:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw2jv7_3h9z0",
        "outputId": "32787145-297c-4107-f57f-e6cb266d1c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CI4pSXRiJny"
      },
      "source": [
        "# Hugging Face Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67d0641b1d0f4d91af49151af7d32803",
            "4a52796a804b428fa7a7072a0a3a02f6",
            "7f903b50f3024078800f6e4cb8ed65bc",
            "081fd38a07724e3ba1b9796a7c69af3b",
            "93815a8be091405484ebb2a08b5d70e0",
            "d642165c5d3e46b19c213294b89e0925",
            "f30cf75ea6814e489284384556638a19",
            "f0c5d94b0f004b41957e453b613f4ca8",
            "fc29a6c5541c4c21ac4dbf1201f6064f",
            "a3ba96aca6dc46118308797ededaad17",
            "ac403506243b4febad35bf7418258177",
            "03eb5ddac1ad4b45854a3780893cd665",
            "795bb521d48c418b9a8dbee2e8541eff",
            "b74d321d037b4205a60dfed9f2f362df",
            "1781eeca65af4e5497b0deda280bcab8",
            "231d4bfc25f84ac0801fb22ee0d1d569",
            "532efc11b55547b490a32d6f59187e68",
            "6b9f8ca77c044fac83cd6ef9b6d4fe83",
            "f232be0d24ed4f38bc16c0de9588f429",
            "db9eae52b88d4a429672daae5371a5c8",
            "0b32ff0622b3478c9aeca22fdf747cee",
            "b06d1825b1d74fdab70532211c7eb40d",
            "03bf518cf4af48e5af4ff25126f772bc",
            "93eeadaa21f146f598ad1f2beb895739",
            "234ff78e659a4c6a863fb8fc5b814266",
            "6a9da7627869497fa87df5e43680e83c",
            "e09e5c0d61d94218a7197ae2e8591b14",
            "b62a5a35722744be902b472c14434d7b",
            "2035ef0194b74c539649ce406848803a",
            "1f3f44bfa12241ddaf05800b2f4e298a",
            "a5a1025f63c0409385e27b53fc46fddb",
            "997fe7f6c950472bb09b0be76aa829b6",
            "3fe3289ba0d64b5987e2c9095fdbe86b",
            "6b75655c3bda4591a0220d613fea8d87",
            "16b4b5907fc24347b0f3b1d79dee3e11",
            "d6fcbbc3e5f64529b8646884bfa04d85",
            "524184e0eb20441480a8598353bb52f7",
            "be1beac4594d4bcb9c3f5fdb4032aa74",
            "2f7e627d64804497aa82eff3fa2e3c3e"
          ]
        },
        "id": "NeYQA_-XReUU",
        "outputId": "3749b10b-e83f-4fe2-eefa-18b9bc2de8da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased and revision 6ea8117 (https://huggingface.co/distilbert/distilbert-base-cased).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 3f49326 (https://huggingface.co/google/vit-base-patch16-224).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67d0641b1d0f4d91af49151af7d32803",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a52796a804b428fa7a7072a0a3a02f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f903b50f3024078800f6e4cb8ed65bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to facebook/detr-resnet-50 and revision 1d5f47b (https://huggingface.co/facebook/detr-resnet-50).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "081fd38a07724e3ba1b9796a7c69af3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.59k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93815a8be091405484ebb2a08b5d70e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d642165c5d3e46b19c213294b89e0925",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f30cf75ea6814e489284384556638a19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/290 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to facebook/detr-resnet-50-panoptic and revision d53b52a (https://huggingface.co/facebook/detr-resnet-50-panoptic).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0c5d94b0f004b41957e453b613f4ca8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc29a6c5541c4c21ac4dbf1201f6064f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/172M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/detr-resnet-50-panoptic were not used when initializing DetrForSegmentation: ['detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing DetrForSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DetrForSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3ba96aca6dc46118308797ededaad17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac403506243b4febad35bf7418258177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03eb5ddac1ad4b45854a3780893cd665",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/172M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "795bb521d48c418b9a8dbee2e8541eff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b74d321d037b4205a60dfed9f2f362df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1781eeca65af4e5497b0deda280bcab8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "231d4bfc25f84ac0801fb22ee0d1d569",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "532efc11b55547b490a32d6f59187e68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to ydshieh/vit-gpt2-coco-en and revision 5bebf1e (https://huggingface.co/ydshieh/vit-gpt2-coco-en).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b9f8ca77c044fac83cd6ef9b6d4fe83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.34k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f232be0d24ed4f38bc16c0de9588f429",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db9eae52b88d4a429672daae5371a5c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.48.3\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b32ff0622b3478c9aeca22fdf747cee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b06d1825b1d74fdab70532211c7eb40d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03bf518cf4af48e5af4ff25126f772bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93eeadaa21f146f598ad1f2beb895739",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "234ff78e659a4c6a863fb8fc5b814266",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a9da7627869497fa87df5e43680e83c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/211 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to google/tapas-base-finetuned-wtq and revision e3dde19 (https://huggingface.co/google/tapas-base-finetuned-wtq).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e09e5c0d61d94218a7197ae2e8591b14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b62a5a35722744be902b472c14434d7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2035ef0194b74c539649ce406848803a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/490 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f3f44bfa12241ddaf05800b2f4e298a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5a1025f63c0409385e27b53fc46fddb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/262k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "997fe7f6c950472bb09b0be76aa829b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "No model was supplied, defaulted to impira/layoutlm-document-qa and revision beed3c4 (https://huggingface.co/impira/layoutlm-document-qa).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fe3289ba0d64b5987e2c9095fdbe86b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b75655c3bda4591a0220d613fea8d87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/511M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16b4b5907fc24347b0f3b1d79dee3e11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/315 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6fcbbc3e5f64529b8646884bfa04d85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "524184e0eb20441480a8598353bb52f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be1beac4594d4bcb9c3f5fdb4032aa74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f7e627d64804497aa82eff3fa2e3c3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n3. Time Series Forecasting: Predicting future values in time series data (not directly supported in the main Transformers library but available through extensions).\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "#---------------------------------------------------#\n",
        "#                     NLP TASKS                     #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Text Classification: Assigning a category to a piece of text.\n",
        "Sentiment Analysis\n",
        "Topic Classification\n",
        "Spam Detection '''\n",
        "\n",
        "classifier = pipeline(\"text-classification\")\n",
        "\n",
        "'''\n",
        "2. Token Classification: Assigning labels to individual tokens in a sequence.\n",
        "Named Entity Recognition (NER)\n",
        "Part-of-Speech Tagging\n",
        "'''\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "\n",
        "'''\n",
        "3. Question Answering: Extracting an answer from a given context based on a question.\n",
        "'''\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "\n",
        "'''\n",
        "4. Text Generation: Generating text based on a given prompt.\n",
        "Language Modeling\n",
        "Story Generation\n",
        "\n",
        "'''\n",
        "\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "'''\n",
        "5. Summarization: Condensing long documents into shorter summaries.\n",
        "'''\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "'''\n",
        "Translation: Translating text from one language to another.\n",
        "'''\n",
        "\n",
        "translator = pipeline(\"translation\",\n",
        "                      model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "'''\n",
        "6. Text2Text Generation: General-purpose text transformation, including summarization and translation.\n",
        "'''\n",
        "\n",
        "text2text_generator = pipeline(\"text2text-generation\")\n",
        "\n",
        "'''\n",
        "7. Fill-Mask: Predicting the masked token in a sequence.\n",
        "'''\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\")\n",
        "\n",
        "'''\n",
        "8. Feature Extraction: Extracting hidden states or features from text.\n",
        "'''\n",
        "\n",
        "feature_extractor = pipeline(\"feature-extraction\")\n",
        "\n",
        "'''\n",
        "9. Sentence Similarity: Measuring the similarity between two sentences.\n",
        "'''\n",
        "#sentence_similarity = pipeline(\"sentence-similarity\")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#             Computer Vision TASKS                 #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Image Classification: Classifying the main content of an image.\n",
        "\n",
        "'''\n",
        "\n",
        "image_classifier = pipeline(\"image-classification\")\n",
        "\n",
        "'''\n",
        "2. Object Detection: Identifying objects within an image and their bounding boxes.\n",
        "'''\n",
        "\n",
        "object_detector = pipeline(\"object-detection\")\n",
        "\n",
        "'''\n",
        "3. Image Segmentation: Segmenting different parts of an image into classes.\n",
        "'''\n",
        "\n",
        "image_segmenter = pipeline(\"image-segmentation\")\n",
        "\n",
        "'''\n",
        "4. Image Generation: Generating images from textual descriptions (using DALL-E or similar models).\n",
        "'''\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#             Speech Processing TASKS               #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. utomatic Speech Recognition (ASR): Converting spoken language into text.\n",
        "'''\n",
        "\n",
        "speech_recognizer = pipeline(\"automatic-speech-recognition\")\n",
        "\n",
        "'''\n",
        "2. Speech Translation: Translating spoken language from one language to another.\n",
        "3. Audio Classification: Classifying audio signals into predefined categories.\n",
        "'''\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#                   Multimodal TASKS                #\n",
        "#---------------------------------------------------#\n",
        "\n",
        "'''\n",
        "1. Image Captioning: Generating a textual description of an image.\n",
        "'''\n",
        "image_captioner = pipeline(\"image-to-text\")\n",
        "'''\n",
        "2. Visual Question Answering (VQA): Answering questions about the content of an image.\n",
        "'''\n",
        "\n",
        "#---------------------------------------------------#\n",
        "#                     Other TASKS                   #\n",
        "#---------------------------------------------------#\n",
        "'''\n",
        "1. Table Question Answering: Answering questions based on tabular data.\n",
        "'''\n",
        "table_qa = pipeline(\"table-question-answering\")\n",
        "\n",
        "'''\n",
        "2. Document Question Answering: Extracting answers from documents like PDFs.\n",
        "\n",
        "'''\n",
        "doc_qa = pipeline(\"document-question-answering\")\n",
        "'''\n",
        "3. Time Series Forecasting: Predicting future values in time series data (not directly supported in the main Transformers library but available through extensions).\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52VnyoTyJFY-"
      },
      "source": [
        "# NLP Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtkS8DKevpNf"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHJpXb2Z7oro",
        "outputId": "26d3991b-b1d6-45a5-a511-93f0cefb78bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9997795224189758}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I was so not happy with the last Mission Impossible Movie\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In6-RXuRoord",
        "outputId": "a34c038e-bb37-44b1-c8d0-805db9aff403"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9992005228996277}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline(task = \"sentiment-analysis\")(\"I was confused with the Barbie Movie\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJYMyOWxnX93",
        "outputId": "61d59956-fbae-4143-88f2-62a172531a68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9964345693588257}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline(task = \"sentiment-analysis\")\\\n",
        "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
        "                                      Lots of them Looks very Promising. \\\n",
        "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
        "                                      There is still lots to do.\\\n",
        "                                      Don't you think?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "0bc93433b1934d5494e29814deb496bc",
            "7b92fec799d642548313ce53cf4fb5ff",
            "0c55ad607643460da507c83eff0e8d10",
            "9ca247eb895d4604a3e89312d87d20e9",
            "65a6cc3fa1fb4e598c1d50caba2fea5b",
            "a41c1a397ad14495be63dc87120cb855"
          ]
        },
        "id": "_J1xtm-wpY6J",
        "outputId": "1d755239-b7a0-4a6b-86cf-0b522d9730d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc93433b1934d5494e29814deb496bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b92fec799d642548313ce53cf4fb5ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c55ad607643460da507c83eff0e8d10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ca247eb895d4604a3e89312d87d20e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65a6cc3fa1fb4e598c1d50caba2fea5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a41c1a397ad14495be63dc87120cb855",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'neutral', 'score': 0.7693331837654114}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline(task = \"sentiment-analysis\", model=\"facebook/bart-large-mnli\")\\\n",
        "                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n",
        "                                      Lots of them Looks very Promising. \\\n",
        "                                      I am not sure if we CAN actually Evaluate LLMs. \\\n",
        "                                      There is still lots to do.\\\n",
        "                                      Don't you think?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrgPSaSgwXJh"
      },
      "source": [
        "### Batch Senteniment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkUfS00hvahT",
        "outputId": "3acf65c7-cf89-4957-ceed-00fe9d199c62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9978686571121216},\n",
              " {'label': 'NEGATIVE', 'score': 0.9995476603507996},\n",
              " {'label': 'NEGATIVE', 'score': 0.9983084201812744},\n",
              " {'label': 'NEGATIVE', 'score': 0.9969881176948547}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\")\n",
        "\n",
        "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
        "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
        "            \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know.\",\\\n",
        "            \"I hate long Meetings.\"]\n",
        "classifier(task_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328,
          "referenced_widgets": [
            "a46c265f48544674bd79d363e4876fca",
            "0ca8aab0487c4fedbf6a4d7caa7977fb",
            "be21b49ebbc24e0cb0a9bbccfb6fc923",
            "ba23af5a283b4a6b87bcbbb4e4deb8d7",
            "25e5fbd3e8ba43f8bc77030bcd9cb6c5",
            "541685c5ea704450be7b7971761447e5",
            "c94da4e38729434d987faf4d6c881895"
          ]
        },
        "id": "aU4plpDUyhqd",
        "outputId": "f5f092f8-6fe0-4fa5-8a66-57622d0bb290"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a46c265f48544674bd79d363e4876fca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ca8aab0487c4fedbf6a4d7caa7977fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be21b49ebbc24e0cb0a9bbccfb6fc923",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba23af5a283b4a6b87bcbbb4e4deb8d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25e5fbd3e8ba43f8bc77030bcd9cb6c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "541685c5ea704450be7b7971761447e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c94da4e38729434d987faf4d6c881895",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'admiration', 'score': 0.7406536340713501},\n",
              " {'label': 'confusion', 'score': 0.9066852331161499},\n",
              " {'label': 'amusement', 'score': 0.9083253145217896},\n",
              " {'label': 'anger', 'score': 0.7870621085166931}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\", model = \"SamLowe/roberta-base-go_emotions\")\n",
        "\n",
        "task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n",
        "            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n",
        "            \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know. It is pretty funny name for a Regression Model.\",\\\n",
        "            \"I hate long Meetings.\"]\n",
        "classifier(task_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH_f9AxZ6LhL"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G46VmgNa73ni",
        "outputId": "776d6521-6a63-468e-9a8e-4668dd823e16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated_text:\n",
            "  Today is a rainy day in London, and we are all tired of the sun.\n",
            "\n",
            "This morning, thousands of protesters from across south London turned up and marched up to the City Hall.\n",
            "This morning at the Old Trafford Hotel, we are\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n",
        "generated_text = text_generator(\"Today is a rainy day in London\",\n",
        "                                truncation=True, #tells the tokenizer to cut off (truncate) the input text if it’s longer than the model's maximum input length.\n",
        "                                num_return_sequences = 2) #tells the model to generate two different outputs (sequences) for the same input prompt.\n",
        "print(\"Generated_text:\\n \", generated_text[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3lydSIe_NvK"
      },
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEA5_fX4-jgC",
        "outputId": "77b1baed-49b0-40a5-b469-49e8cc1c851b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.7823827266693115,\n",
              " 'start': 5,\n",
              " 'end': 25,\n",
              " 'answer': 'developing AI models'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline(\"question-answering\")\n",
        "question = \"What is my job?\"\n",
        "context = \"I am developing AI models with Python.\"\n",
        "qa_model(question = question, context = context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRN2QYnq0Ex2"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM8irpMIPKhQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX7Tpn-HErlo",
        "outputId": "a4d95028-1f13-43ce-a603-7f2a215345c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': '2 stars', 'score': 0.5099301934242249}]\n"
          ]
        }
      ],
      "source": [
        "model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "mymodel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\n",
        "mytokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model = mymodel2 , tokenizer = mytokenizer2)\n",
        "res = classifier(\"I was so not happy with the Barbie Movie\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "6d543c1c5d96431db37fe5b2a19c6332",
            "d074abf5b15c4702b900d53e85b11186",
            "5375727388e44425ae8963aef35a587a",
            "2d780b25e372447ba46bf267b14e3be7"
          ]
        },
        "id": "VO4PcjquK9WK",
        "outputId": "6a352c87-f2a3-4d70-d1fb-e8061d3459c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d543c1c5d96431db37fe5b2a19c6332",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d074abf5b15c4702b900d53e85b11186",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5375727388e44425ae8963aef35a587a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d780b25e372447ba46bf267b14e3be7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['i', 'was', 'so', 'not', 'happy', 'with', 'the', 'barbie', 'movie']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example text\n",
        "text = \"I was so not happy with the Barbie Movie\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHOdgrtaP-2n",
        "outputId": "8314d185-be83-4c65-af8e-5a75ef2abb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: [1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185]\n"
          ]
        }
      ],
      "source": [
        "# Convert tokens to input IDs\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Input IDs:\", input_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OqffWefQLPA",
        "outputId": "57895a4b-d702-449b-fdc9-ebb559a4b8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded Input: {'input_ids': [101, 1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "# Encode the text (tokenization + converting to input IDs)\n",
        "encoded_input = tokenizer(text)\n",
        "print(\"Encoded Input:\", encoded_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx35mzBBPyYa",
        "outputId": "dbc69566-5018-4854-c171-fad4f5a64ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decode Output:  i was so not happy with the barbie movie\n"
          ]
        }
      ],
      "source": [
        "# Decode the text\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decode Output: \", decoded_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "7aa525c9f2cf467d841edae2f8d05223",
            "41ae8fa1b5ad45d8b5422b34668371b0",
            "0f7ca33af052472cba88701e02e18010",
            "da07d81650f941c98a4f43193d5ec8ac"
          ]
        },
        "id": "boghHype_R5R",
        "outputId": "705e833b-d888-4cb9-bcaf-4fe601874ee0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aa525c9f2cf467d841edae2f8d05223",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41ae8fa1b5ad45d8b5422b34668371b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f7ca33af052472cba88701e02e18010",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da07d81650f941c98a4f43193d5ec8ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['I', 'was', 'so', 'not', 'happy', 'with', 'the', 'Barbie', 'Movie']\n",
            "Input IDs: [146, 1108, 1177, 1136, 2816, 1114, 1103, 25374, 8275]\n",
            "Encoded Input: {'input_ids': [101, 146, 1108, 1177, 1136, 2816, 1114, 1103, 25374, 8275, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Decode Output:  I was so not happy with the Barbie Movie\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Example text\n",
        "text = \"I was so not happy with the Barbie Movie\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to input IDs\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "# Encode the text (tokenization + converting to input IDs)\n",
        "encoded_input = tokenizer(text)\n",
        "print(\"Encoded Input:\", encoded_input)\n",
        "\n",
        "# Decode the text\n",
        "decoded_output = tokenizer.decode(input_ids)\n",
        "print(\"Decode Output: \", decoded_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa5YaOLhLarj"
      },
      "source": [
        "**token_type_ids**<br>\n",
        "These IDs are used to distinguish between different sequences in tasks that involve multiple sentences, such as question-answering and sentence-pair classification. BERT uses this mechanism to understand which tokens belong to which segment. For single-sequence tasks like sentiment analysis, token_type_ids are all zeros.\n",
        "\n",
        "**attention_mask** <br>\n",
        "The attention mask is used to differentiate between actual tokens and padding tokens (if any). It helps the model focus on non-padding tokens and ignore padding tokens. A value of 1 indicates that the token should be attended to, while a value of 0 indicates padding.\n",
        "\n",
        "**Why Padding Tokens Are Used**<br>\n",
        "Uniform Sequence Length: Deep learning models typically process input data in batches. To efficiently process these batches, all sequences in a batch must have the same length. Padding tokens ensure this by extending shorter sequences to match the length of the longest sequence in the batch.\n",
        "Efficient Computation: Fixed-length sequences allow for more efficient use of hardware resources, as the model can process all sequences in parallel without needing to handle variable-length sequences individually.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXen6cV5ikd5"
      },
      "source": [
        "# Fine Tunning IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qasZweQti8v8"
      },
      "source": [
        "## Step 1: Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN9QQ9l3imOk",
        "outputId": "b2b7135c-22a9-43c1-9e23-3f7c11739d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets #hugging face also has datasets available in its platform\n",
        "#only executing this and no fsspec was giving error so run next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x6XBBR8gl-Gx",
        "outputId": "a21739ac-d312-47b2-a409-52ddd8f0784c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: fsspec==2023.10.0 in /usr/local/lib/python3.11/dist-packages (2023.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets==2.18.0 fsspec==2023.10.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9oxKxXbi_ok"
      },
      "source": [
        "## Step 2: Load and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "f34c6f71436340769f6db4e1ef5ece70",
            "2bd22dc729ff48358383e2e6922b7d87",
            "7858585f75224a578329598bc9c2a708",
            "8edbb20906344ce58ab9e7a108e6d5e6",
            "2eb348d01d0e41a2972d2fad31c8a22f",
            "44999316800745f9b747980268bd2204"
          ]
        },
        "id": "cwPfQ9qdk9fa",
        "outputId": "503d710a-84ec-42b5-b5f2-73057e3dc0c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f34c6f71436340769f6db4e1ef5ece70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bd22dc729ff48358383e2e6922b7d87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7858585f75224a578329598bc9c2a708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8edbb20906344ce58ab9e7a108e6d5e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2eb348d01d0e41a2972d2fad31c8a22f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44999316800745f9b747980268bd2204",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('imdb')\n",
        "#(imbd dataset) go to datasets section in hugging face and search imbd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54Hhfjlul8Y6",
        "outputId": "eda03e33-f2c1-4711-e58f-fb25811b84df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnQGMk5MSTZJ",
        "outputId": "f886db40-cf9f-4af5-93de-2963cba90bb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qII9CiJlM5D"
      },
      "source": [
        "## Step 3: Preprocess the Data\n",
        "Tokenize the dataset using the tokenizer associated with the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "bf9a65081b514b2a82fc4ee3de0bbf29",
            "9d96ceebb21b4310830e239b271fef75",
            "0df5ccfe5c5a41fe94ff041f513baef2"
          ]
        },
        "id": "xu7u5yZTiuV8",
        "outputId": "14023c48-ce8f-4159-f0b9-c800ae812003"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf9a65081b514b2a82fc4ee3de0bbf29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d96ceebb21b4310830e239b271fef75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df5ccfe5c5a41fe94ff041f513baef2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "    # text in examples['text'] because in our dataset, see above, the whole content is inside 'text:'\n",
        "    # When batched=True, the map() function processes multiple examples at once rather than one by one. It means examples['text'] will be a list of strings instead of a single string.\n",
        "    # padding=\"max_length\" tells the tokenizer to pad all sequences to the model’s maximum input length\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEpqrVlamLyn",
        "outputId": "808ce78d-f12e-49e4-e9b1-be5f53993319"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6-T8DcXTQOP",
        "outputId": "b7feb5c5-69d4-4527-e7c6-0b9c8c65459e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              " 'label': 0,\n",
              " 'input_ids': [101,\n",
              "  1045,\n",
              "  12524,\n",
              "  1045,\n",
              "  2572,\n",
              "  8025,\n",
              "  1011,\n",
              "  3756,\n",
              "  2013,\n",
              "  2026,\n",
              "  2678,\n",
              "  3573,\n",
              "  2138,\n",
              "  1997,\n",
              "  2035,\n",
              "  1996,\n",
              "  6704,\n",
              "  2008,\n",
              "  5129,\n",
              "  2009,\n",
              "  2043,\n",
              "  2009,\n",
              "  2001,\n",
              "  2034,\n",
              "  2207,\n",
              "  1999,\n",
              "  3476,\n",
              "  1012,\n",
              "  1045,\n",
              "  2036,\n",
              "  2657,\n",
              "  2008,\n",
              "  2012,\n",
              "  2034,\n",
              "  2009,\n",
              "  2001,\n",
              "  8243,\n",
              "  2011,\n",
              "  1057,\n",
              "  1012,\n",
              "  1055,\n",
              "  1012,\n",
              "  8205,\n",
              "  2065,\n",
              "  2009,\n",
              "  2412,\n",
              "  2699,\n",
              "  2000,\n",
              "  4607,\n",
              "  2023,\n",
              "  2406,\n",
              "  1010,\n",
              "  3568,\n",
              "  2108,\n",
              "  1037,\n",
              "  5470,\n",
              "  1997,\n",
              "  3152,\n",
              "  2641,\n",
              "  1000,\n",
              "  6801,\n",
              "  1000,\n",
              "  1045,\n",
              "  2428,\n",
              "  2018,\n",
              "  2000,\n",
              "  2156,\n",
              "  2023,\n",
              "  2005,\n",
              "  2870,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1996,\n",
              "  5436,\n",
              "  2003,\n",
              "  8857,\n",
              "  2105,\n",
              "  1037,\n",
              "  2402,\n",
              "  4467,\n",
              "  3689,\n",
              "  3076,\n",
              "  2315,\n",
              "  14229,\n",
              "  2040,\n",
              "  4122,\n",
              "  2000,\n",
              "  4553,\n",
              "  2673,\n",
              "  2016,\n",
              "  2064,\n",
              "  2055,\n",
              "  2166,\n",
              "  1012,\n",
              "  1999,\n",
              "  3327,\n",
              "  2016,\n",
              "  4122,\n",
              "  2000,\n",
              "  3579,\n",
              "  2014,\n",
              "  3086,\n",
              "  2015,\n",
              "  2000,\n",
              "  2437,\n",
              "  2070,\n",
              "  4066,\n",
              "  1997,\n",
              "  4516,\n",
              "  2006,\n",
              "  2054,\n",
              "  1996,\n",
              "  2779,\n",
              "  25430,\n",
              "  14728,\n",
              "  2245,\n",
              "  2055,\n",
              "  3056,\n",
              "  2576,\n",
              "  3314,\n",
              "  2107,\n",
              "  2004,\n",
              "  1996,\n",
              "  5148,\n",
              "  2162,\n",
              "  1998,\n",
              "  2679,\n",
              "  3314,\n",
              "  1999,\n",
              "  1996,\n",
              "  2142,\n",
              "  2163,\n",
              "  1012,\n",
              "  1999,\n",
              "  2090,\n",
              "  4851,\n",
              "  8801,\n",
              "  1998,\n",
              "  6623,\n",
              "  7939,\n",
              "  4697,\n",
              "  3619,\n",
              "  1997,\n",
              "  8947,\n",
              "  2055,\n",
              "  2037,\n",
              "  10740,\n",
              "  2006,\n",
              "  4331,\n",
              "  1010,\n",
              "  2016,\n",
              "  2038,\n",
              "  3348,\n",
              "  2007,\n",
              "  2014,\n",
              "  3689,\n",
              "  3836,\n",
              "  1010,\n",
              "  19846,\n",
              "  1010,\n",
              "  1998,\n",
              "  2496,\n",
              "  2273,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  2054,\n",
              "  8563,\n",
              "  2033,\n",
              "  2055,\n",
              "  1045,\n",
              "  2572,\n",
              "  8025,\n",
              "  1011,\n",
              "  3756,\n",
              "  2003,\n",
              "  2008,\n",
              "  2871,\n",
              "  2086,\n",
              "  3283,\n",
              "  1010,\n",
              "  2023,\n",
              "  2001,\n",
              "  2641,\n",
              "  26932,\n",
              "  1012,\n",
              "  2428,\n",
              "  1010,\n",
              "  1996,\n",
              "  3348,\n",
              "  1998,\n",
              "  16371,\n",
              "  25469,\n",
              "  5019,\n",
              "  2024,\n",
              "  2261,\n",
              "  1998,\n",
              "  2521,\n",
              "  2090,\n",
              "  1010,\n",
              "  2130,\n",
              "  2059,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  2025,\n",
              "  2915,\n",
              "  2066,\n",
              "  2070,\n",
              "  10036,\n",
              "  2135,\n",
              "  2081,\n",
              "  22555,\n",
              "  2080,\n",
              "  1012,\n",
              "  2096,\n",
              "  2026,\n",
              "  2406,\n",
              "  3549,\n",
              "  2568,\n",
              "  2424,\n",
              "  2009,\n",
              "  16880,\n",
              "  1010,\n",
              "  1999,\n",
              "  4507,\n",
              "  3348,\n",
              "  1998,\n",
              "  16371,\n",
              "  25469,\n",
              "  2024,\n",
              "  1037,\n",
              "  2350,\n",
              "  18785,\n",
              "  1999,\n",
              "  4467,\n",
              "  5988,\n",
              "  1012,\n",
              "  2130,\n",
              "  13749,\n",
              "  7849,\n",
              "  24544,\n",
              "  1010,\n",
              "  15835,\n",
              "  2037,\n",
              "  3437,\n",
              "  2000,\n",
              "  2204,\n",
              "  2214,\n",
              "  2879,\n",
              "  2198,\n",
              "  4811,\n",
              "  1010,\n",
              "  2018,\n",
              "  3348,\n",
              "  5019,\n",
              "  1999,\n",
              "  2010,\n",
              "  3152,\n",
              "  1012,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1026,\n",
              "  7987,\n",
              "  1013,\n",
              "  1028,\n",
              "  1045,\n",
              "  2079,\n",
              "  4012,\n",
              "  3549,\n",
              "  2094,\n",
              "  1996,\n",
              "  16587,\n",
              "  2005,\n",
              "  1996,\n",
              "  2755,\n",
              "  2008,\n",
              "  2151,\n",
              "  3348,\n",
              "  3491,\n",
              "  1999,\n",
              "  1996,\n",
              "  2143,\n",
              "  2003,\n",
              "  3491,\n",
              "  2005,\n",
              "  6018,\n",
              "  5682,\n",
              "  2738,\n",
              "  2084,\n",
              "  2074,\n",
              "  2000,\n",
              "  5213,\n",
              "  2111,\n",
              "  1998,\n",
              "  2191,\n",
              "  2769,\n",
              "  2000,\n",
              "  2022,\n",
              "  3491,\n",
              "  1999,\n",
              "  26932,\n",
              "  12370,\n",
              "  1999,\n",
              "  2637,\n",
              "  1012,\n",
              "  1045,\n",
              "  2572,\n",
              "  8025,\n",
              "  1011,\n",
              "  3756,\n",
              "  2003,\n",
              "  1037,\n",
              "  2204,\n",
              "  2143,\n",
              "  2005,\n",
              "  3087,\n",
              "  5782,\n",
              "  2000,\n",
              "  2817,\n",
              "  1996,\n",
              "  6240,\n",
              "  1998,\n",
              "  14629,\n",
              "  1006,\n",
              "  2053,\n",
              "  26136,\n",
              "  3832,\n",
              "  1007,\n",
              "  1997,\n",
              "  4467,\n",
              "  5988,\n",
              "  1012,\n",
              "  2021,\n",
              "  2428,\n",
              "  1010,\n",
              "  2023,\n",
              "  2143,\n",
              "  2987,\n",
              "  1005,\n",
              "  1056,\n",
              "  2031,\n",
              "  2172,\n",
              "  1997,\n",
              "  1037,\n",
              "  5436,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'token_type_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gLPJMBsld8k"
      },
      "source": [
        "## Step 4: Set Up the Training Arguments\n",
        "Specify the hyperparameters and training settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MWj9S2CmKPN",
        "outputId": "e844cc1b-94dd-4590-f669-96c6cb193b4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "auto_find_batch_size=False,\n",
              "average_tokens_across_devices=False,\n",
              "batch_eval_metrics=False,\n",
              "bf16=False,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_persistent_workers=False,\n",
              "dataloader_pin_memory=True,\n",
              "dataloader_prefetch_factor=None,\n",
              "ddp_backend=None,\n",
              "ddp_broadcast_buffers=None,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "ddp_timeout=1800,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "do_eval=True,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_do_concat_batches=True,\n",
              "eval_on_start=False,\n",
              "eval_steps=None,\n",
              "eval_strategy=IntervalStrategy.EPOCH,\n",
              "eval_use_gather_object=False,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "fsdp=[],\n",
              "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
              "fsdp_min_num_params=0,\n",
              "fsdp_transformer_layer_cls_to_wrap=None,\n",
              "full_determinism=False,\n",
              "gradient_accumulation_steps=1,\n",
              "gradient_checkpointing=False,\n",
              "gradient_checkpointing_kwargs=None,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_always_push=False,\n",
              "hub_model_id=None,\n",
              "hub_private_repo=None,\n",
              "hub_strategy=HubStrategy.EVERY_SAVE,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "include_for_metrics=[],\n",
              "include_inputs_for_metrics=False,\n",
              "include_num_input_tokens_seen=False,\n",
              "include_tokens_per_second=False,\n",
              "jit_mode_eval=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=2e-05,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=0,\n",
              "log_level=passive,\n",
              "log_level_replica=warning,\n",
              "log_on_each_node=True,\n",
              "logging_dir=./results/runs/Jun11_07-28-14_ed7e52833193,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=500,\n",
              "logging_strategy=IntervalStrategy.STEPS,\n",
              "lr_scheduler_kwargs={},\n",
              "lr_scheduler_type=SchedulerType.LINEAR,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "neftune_noise_alpha=None,\n",
              "no_cuda=False,\n",
              "num_train_epochs=1,\n",
              "optim=OptimizerNames.ADAMW_TORCH,\n",
              "optim_args=None,\n",
              "optim_target_modules=None,\n",
              "output_dir=./results,\n",
              "overwrite_output_dir=False,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=16,\n",
              "per_device_train_batch_size=16,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "ray_scope=last,\n",
              "remove_unused_columns=True,\n",
              "report_to=['tensorboard'],\n",
              "restore_callback_states_from_checkpoint=False,\n",
              "resume_from_checkpoint=None,\n",
              "run_name=./results,\n",
              "save_on_each_node=False,\n",
              "save_only_model=False,\n",
              "save_safetensors=True,\n",
              "save_steps=500,\n",
              "save_strategy=SaveStrategy.STEPS,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "skip_memory_metrics=True,\n",
              "tf32=None,\n",
              "torch_compile=False,\n",
              "torch_compile_backend=None,\n",
              "torch_compile_mode=None,\n",
              "torch_empty_cache_steps=None,\n",
              "torchdynamo=None,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_cpu=False,\n",
              "use_ipex=False,\n",
              "use_legacy_prediction_loop=False,\n",
              "use_liger_kernel=False,\n",
              "use_mps_device=False,\n",
              "warmup_ratio=0.0,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.01,\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    eval_strategy =\"epoch\",     # Evaluate every epoch\n",
        "    learning_rate=2e-5,              # Learning rate\n",
        "    per_device_train_batch_size=16,  # Batch size for training\n",
        "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
        "    num_train_epochs=1,              # Number of training epochs\n",
        "    weight_decay=0.01,               # Strength of weight decay\n",
        "    report_to=None\n",
        ")\n",
        "training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3Jjk0amlfV"
      },
      "source": [
        "## Step 5: Initialize the Model\n",
        "Load the pre-trained model and define the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I346Iwb64lGS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "778fbc5d436c4e5d873f057d765a8c54"
          ]
        },
        "id": "w7aOUUEbmpuQ",
        "outputId": "aabc7a8d-8192-4a42-a1d3-b3f7ade87332"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "778fbc5d436c4e5d873f057d765a8c54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test']\n",
        "    # training_args, tokenized_datasets are defined above\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt3aLUxwqwv2"
      },
      "source": [
        "## Step 6: Train the Model\n",
        "Fine-tune the pre-trained model on your specific dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "J-Bw4XHMqtOW",
        "outputId": "5202a05f-9ce2-435b-a9af-8c14b97fb5b5"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3081630584>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_xla_v2_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_spmd_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainer: training requires a train_dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         return self._get_dataloader(\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_get_dataloader\u001b[0;34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mdataloader_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                 )\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEEPSPEED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m             \u001b[0mmodel_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mdistributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistributed_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/state.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;31m# so we just modify the error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_attrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1191\u001b[0m                 \u001b[0;34mf\"`AcceleratorState` object has no attribute `{name}`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0;34m\"This happens if `AcceleratorState._reset_state()` was called and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6xiB0b1q5Qr"
      },
      "source": [
        "## Step 7: Evaluate the Model\n",
        "Assess the model's performance on a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7f0xjMHGq6nD",
        "outputId": "40e30fcd-7919-4112-c35e-2ff1fe329447"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 13:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.17594651877880096, 'eval_runtime': 784.9264, 'eval_samples_per_second': 31.85, 'eval_steps_per_second': 1.991, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CikT76trAUZ"
      },
      "source": [
        "## Step 8: Save the Fine-Tuned Model\n",
        "Save the fine-tuned model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QguzOw8erBlx",
        "outputId": "0c4d4e6b-a7c8-4b6b-f415-f5176ef6a235"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./fine-tuned-tokenizer/tokenizer_config.json',\n",
              " './fine-tuned-tokenizer/special_tokens_map.json',\n",
              " './fine-tuned-tokenizer/vocab.txt',\n",
              " './fine-tuned-tokenizer/added_tokens.json',\n",
              " './fine-tuned-tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save_pretrained('./fine-tuned-model')\n",
        "tokenizer.save_pretrained('./fine-tuned-tokenizer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvc5VMWlie_G"
      },
      "source": [
        "# ArXiv Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58Ak1nFiasi",
        "outputId": "59b3440a-cfb4-46ac-e659-686d0e1f726c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.7.4)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=af1ddfe41a100f457af12df8d9116f91b1be5084b122cff4d27211b8fb20df0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk7PgqmgI_6h"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hLYntEV4JVYx",
        "outputId": "c681166a-2969-4f5d-8f53-1e10809045df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-5f6bc63cd8db>:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-08-30 17:11:36+00:00\",\n        \"max\": \"2024-08-30 17:55:05+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2024-08-30 17:12:14+00:00\",\n          \"2024-08-30 17:52:55+00:00\",\n          \"2024-08-30 17:28:48+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Open-vocabulary Temporal Action Localization using VLMs\",\n          \"Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding\",\n          \"Virtual EVE: a Deep Learning Model for Solar Irradiance Prediction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Video action localization aims to find timings of a specific action from a\\nlong video. Although existing learning-based approaches have been successful,\\nthose require annotating videos that come with a considerable labor cost. This\\npaper proposes a learning-free, open-vocabulary approach based on emerging\\noff-the-shelf vision-language models (VLM). The challenge stems from the fact\\nthat VLMs are neither designed to process long videos nor tailored for finding\\nactions. We overcome these problems by extending an iterative visual prompting\\ntechnique. Specifically, we sample video frames into a concatenated image with\\nframe index labels, making a VLM guess a frame that is considered to be closest\\nto the start/end of the action. Iterating this process by narrowing a sampling\\ntime window results in finding a specific frame of start and end of an action.\\nWe demonstrate that this sampling technique yields reasonable results,\\nillustrating a practical extension of VLMs for understanding videos. A sample\\ncode is available at\\nhttps://microsoft.github.io/VLM-Video-Action-Localization/.\",\n          \"While existing research often treats long-form videos as extended short\\nvideos, we propose a novel approach that more accurately reflects human\\ncognition. This paper introduces BREASE: BRidging Episodes And SEmantics for\\nLong-Form Video Understanding, a model that simulates episodic memory\\naccumulation to capture action sequences and reinforces them with semantic\\nknowledge dispersed throughout the video. Our work makes two key contributions:\\nFirst, we develop an Episodic COmpressor (ECO) that efficiently aggregates\\ncrucial representations from micro to semi-macro levels. Second, we propose a\\nSemantics reTRiever (SeTR) that enhances these aggregated representations with\\nsemantic information by focusing on the broader context, dramatically reducing\\nfeature dimensionality while preserving relevant macro-level information.\\nExtensive experiments demonstrate that BREASE achieves state-of-the-art\\nperformance across multiple long video understanding benchmarks in both\\nzero-shot and fully-supervised settings. The project page and code are at:\\nhttps://joslefaure.github.io/assets/html/hermes.html.\",\n          \"Understanding space weather is vital for the protection of our terrestrial\\nand space infrastructure. In order to predict space weather accurately, large\\namounts of data are required, particularly in the extreme ultraviolet (EUV)\\nspectrum. An exquisite source of information for such data is provided by the\\nSolar Dynamic Observatory (SDO), which has been gathering solar measurements\\nfor the past 13 years. However, after a malfunction in 2014 affecting the\\nonboard Multiple EUV Grating Spectrograph A (MEGS-A) instrument, the scientific\\noutput in terms of EUV measurements has been significantly degraded. Building\\nupon existing research, we propose to utilize deep learning for the\\nvirtualization of the defective instrument. Our architecture features a linear\\ncomponent and a convolutional neural network (CNN) -- with EfficientNet as a\\nbackbone. The architecture utilizes as input grayscale images of the Sun at\\nmultiple frequencies -- provided by the Atmospheric Imaging Assembly (AIA) --\\nas well as solar magnetograms produced by the Helioseismic and Magnetic Imager\\n(HMI). Our findings highlight how AIA data are all that is needed for accurate\\npredictions of solar irradiance. Additionally, our model constitutes an\\nimprovement with respect to the state-of-the-art in the field, further\\npromoting the idea of deep learning as a viable option for the virtualization\\nof scientific instruments.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-99cbd294-a0ce-4a1d-9bd8-e9275c3a73ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-08-30 17:55:05+00:00</td>\n",
              "      <td>The picasso gas model: Painting intracluster gas on gravity-only simulations</td>\n",
              "      <td>We introduce picasso, a model designed to predict thermodynamic properties of\\nthe intracluster medium based on the properties of halos in gravity-only\\nsimulations. The predictions result from the combination of an analytical gas\\nmodel, mapping gas properties to the gravitational potential, and of a machine\\nlearning model to predict the model parameters for individual halos based on\\ntheir scalar properties, such as mass and concentration. Once trained, the\\nmodel can be applied to make predictions for arbitrary potential distributions,\\nallowing its use with flexible inputs such as N-body particle distributions or\\nradial profiles. We present the model, and train it using pairs of gravity-only\\nand hydrodynamic simulations. We show that when trained on non-radiative\\nhydrodynamic simulations, picasso can make remarkably accurate and precise\\npredictions of intracluster gas thermodynamics. Training the model on\\nfull-physics simulations yields robust predictions as well, albeit with\\nslightly degraded performance. We further show that the model can be trained to\\nmake accurate predictions from very minimal information, at the cost of\\nmodestly reduced precision. picasso is made publicly available as a Python\\npackage, which includes trained models that can be used to make predictions\\neasily and efficiently, in a fully auto-differentiable and hardware-accelerated\\nframework.</td>\n",
              "      <td>[astro-ph.CO]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-08-30 17:52:55+00:00</td>\n",
              "      <td>Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding</td>\n",
              "      <td>While existing research often treats long-form videos as extended short\\nvideos, we propose a novel approach that more accurately reflects human\\ncognition. This paper introduces BREASE: BRidging Episodes And SEmantics for\\nLong-Form Video Understanding, a model that simulates episodic memory\\naccumulation to capture action sequences and reinforces them with semantic\\nknowledge dispersed throughout the video. Our work makes two key contributions:\\nFirst, we develop an Episodic COmpressor (ECO) that efficiently aggregates\\ncrucial representations from micro to semi-macro levels. Second, we propose a\\nSemantics reTRiever (SeTR) that enhances these aggregated representations with\\nsemantic information by focusing on the broader context, dramatically reducing\\nfeature dimensionality while preserving relevant macro-level information.\\nExtensive experiments demonstrate that BREASE achieves state-of-the-art\\nperformance across multiple long video understanding benchmarks in both\\nzero-shot and fully-supervised settings. The project page and code are at:\\nhttps://joslefaure.github.io/assets/html/hermes.html.</td>\n",
              "      <td>[cs.CV, cs.AI, cs.CL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-08-30 17:35:06+00:00</td>\n",
              "      <td>DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model</td>\n",
              "      <td>Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\\nreconstruction and visualization. While foundation models like Depth Anything\\nModels (DAM) show promise, directly applying them to surgery often yields\\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\\noverfitting and catastrophic forgetting, compromising model robustness and\\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\\nissues, its uniform parameter distribution neglects the inherent feature\\nhierarchy, where earlier layers, learning more general features, require more\\nparameters than later ones. To tackle this issue, we introduce Depth Anything\\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\\nearlier layers and gradually decreasing parameters in later layers. We also\\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\\nperception by better tailoring the foundation model to the specific\\nrequirements of the surgical environment. The proposed method is validated on\\nthe SCARED dataset and demonstrates superior performance over recent\\nstate-of-the-art self-supervised monocular depth estimation techniques,\\nachieving an improvement of 13.3% in the absolute relative error metric. The\\ncode and pre-trained weights are available at\\nhttps://github.com/mobarakol/DARES.</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-08-30 17:34:46+00:00</td>\n",
              "      <td>SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection</td>\n",
              "      <td>Synthesizing the voices of unseen speakers is a persisting challenge in\\nmulti-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on\\nmodeling speaker characteristics through speaker conditioning during training.\\nModeling unseen speaker attributes through this approach has necessitated an\\nincrease in model complexity, which makes it challenging to reproduce results\\nand improve upon them. We design a simple alternative to this. We propose\\nSelectTTS, a novel method to select the appropriate frames from the target\\nspeaker and decode using frame-level self-supervised learning (SSL) features.\\nWe show that this approach can effectively capture speaker characteristics for\\nunseen speakers, and achieves comparable results to other multi-speaker TTS\\nframeworks in both objective and subjective metrics. With SelectTTS, we show\\nthat frame selection from the target speaker's speech is a direct way to\\nachieve generalization in unseen speakers with low model complexity. We achieve\\nbetter speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E\\nwith over an 8x reduction in model parameters and a 270x reduction in training\\ndata</td>\n",
              "      <td>[eess.AS, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-08-30 17:29:25+00:00</td>\n",
              "      <td>Advancing Multi-talker ASR Performance with Large Language Models</td>\n",
              "      <td>Recognizing overlapping speech from multiple speakers in conversational\\nscenarios is one of the most challenging problem for automatic speech\\nrecognition (ASR). Serialized output training (SOT) is a classic method to\\naddress multi-talker ASR, with the idea of concatenating transcriptions from\\nmultiple speakers according to the emission times of their speech for training.\\nHowever, SOT-style transcriptions, derived from concatenating multiple related\\nutterances in a conversation, depend significantly on modeling long contexts.\\nTherefore, compared to traditional methods that primarily emphasize encoder\\nperformance in attention-based encoder-decoder (AED) architectures, a novel\\napproach utilizing large language models (LLMs) that leverages the capabilities\\nof pre-trained decoders may be better suited for such complex and challenging\\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\\nmulti-talker dataset using appropriate strategies. Experimental results\\ndemonstrate that our approach surpasses traditional AED-based methods on the\\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\\nevaluation set of the real-world dataset AMI, outperforming the AED model\\ntrained with 1000 times more supervised data in previous works.</td>\n",
              "      <td>[eess.AS, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-08-30 17:28:48+00:00</td>\n",
              "      <td>Virtual EVE: a Deep Learning Model for Solar Irradiance Prediction</td>\n",
              "      <td>Understanding space weather is vital for the protection of our terrestrial\\nand space infrastructure. In order to predict space weather accurately, large\\namounts of data are required, particularly in the extreme ultraviolet (EUV)\\nspectrum. An exquisite source of information for such data is provided by the\\nSolar Dynamic Observatory (SDO), which has been gathering solar measurements\\nfor the past 13 years. However, after a malfunction in 2014 affecting the\\nonboard Multiple EUV Grating Spectrograph A (MEGS-A) instrument, the scientific\\noutput in terms of EUV measurements has been significantly degraded. Building\\nupon existing research, we propose to utilize deep learning for the\\nvirtualization of the defective instrument. Our architecture features a linear\\ncomponent and a convolutional neural network (CNN) -- with EfficientNet as a\\nbackbone. The architecture utilizes as input grayscale images of the Sun at\\nmultiple frequencies -- provided by the Atmospheric Imaging Assembly (AIA) --\\nas well as solar magnetograms produced by the Helioseismic and Magnetic Imager\\n(HMI). Our findings highlight how AIA data are all that is needed for accurate\\npredictions of solar irradiance. Additionally, our model constitutes an\\nimprovement with respect to the state-of-the-art in the field, further\\npromoting the idea of deep learning as a viable option for the virtualization\\nof scientific instruments.</td>\n",
              "      <td>[astro-ph.SR, astro-ph.IM]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2024-08-30 17:26:46+00:00</td>\n",
              "      <td>Neural network-based closure models for large-eddy simulations with explicit filtering</td>\n",
              "      <td>Data from direct numerical simulations of turbulent flows are commonly used\\nto train neural network-based models as subgrid closures for large-eddy\\nsimulations; however, models with low a priori accuracy have been observed to\\nfortuitously provide better a posteriori results than models with high a priori\\naccuracy. This anomaly can be traced to a dataset shift in the learning\\nproblem, arising from inconsistent filtering in the training and testing\\nstages. We propose a resolution to this issue that uses explicit filtering of\\nthe nonlinear advection term in the large-eddy simulation momentum equations to\\ncontrol aliasing errors. Within the context of explicitly-filtered large-eddy\\nsimulations, we develop neural network-based models for which a priori accuracy\\nis a good predictor of a posteriori performance. We evaluate the proposed\\nmethod in a large-eddy simulation of a turbulent flow in a plane channel at\\n$Re_{\\tau} = 180$. Our findings show that an explicitly-filtered large-eddy\\nsimulation with a filter-to-grid ratio of 2 sufficiently controls the numerical\\nerrors so as to allow for accurate and stable simulations.</td>\n",
              "      <td>[physics.flu-dyn, physics.comp-ph]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2024-08-30 17:16:18+00:00</td>\n",
              "      <td>CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion</td>\n",
              "      <td>With advancements in video generative AI models (e.g., SORA), creators are\\nincreasingly using these techniques to enhance video previsualization. However,\\nthey face challenges with incomplete and mismatched AI workflows. Existing\\nmethods mainly rely on text descriptions and struggle with camera placement, a\\nkey component of previsualization. To address these issues, we introduce\\nCinePreGen, a visual previsualization system enhanced with engine-powered\\ndiffusion. It features a novel camera and storyboard interface that offers\\ndynamic control, from global to local camera adjustments. This is combined with\\na user-friendly AI rendering workflow, which aims to achieve consistent results\\nthrough multi-masked IP-Adapter and engine simulation guidelines. In our\\ncomprehensive evaluation study, we demonstrate that our system reduces\\ndevelopment viscosity (i.e., the complexity and challenges in the development\\nprocess), meets users' needs for extensive control and iteration in the design\\nprocess, and outperforms other AI video production workflows in cinematic\\ncamera movement, as shown by our experiments and a within-subjects user study.\\nWith its intuitive camera controls and realistic rendering of camera motion,\\nCinePreGen shows great potential for improving video production for both\\nindividual creators and industry professionals.</td>\n",
              "      <td>[cs.CV, cs.HC]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2024-08-30 17:12:14+00:00</td>\n",
              "      <td>Open-vocabulary Temporal Action Localization using VLMs</td>\n",
              "      <td>Video action localization aims to find timings of a specific action from a\\nlong video. Although existing learning-based approaches have been successful,\\nthose require annotating videos that come with a considerable labor cost. This\\npaper proposes a learning-free, open-vocabulary approach based on emerging\\noff-the-shelf vision-language models (VLM). The challenge stems from the fact\\nthat VLMs are neither designed to process long videos nor tailored for finding\\nactions. We overcome these problems by extending an iterative visual prompting\\ntechnique. Specifically, we sample video frames into a concatenated image with\\nframe index labels, making a VLM guess a frame that is considered to be closest\\nto the start/end of the action. Iterating this process by narrowing a sampling\\ntime window results in finding a specific frame of start and end of an action.\\nWe demonstrate that this sampling technique yields reasonable results,\\nillustrating a practical extension of VLMs for understanding videos. A sample\\ncode is available at\\nhttps://microsoft.github.io/VLM-Video-Action-Localization/.</td>\n",
              "      <td>[cs.CV, cs.AI, cs.RO]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2024-08-30 17:11:36+00:00</td>\n",
              "      <td>Generative AI Enables Medical Image Segmentation in Ultra Low-Data Regimes</td>\n",
              "      <td>Semantic segmentation of medical images is pivotal in applications like\\ndisease diagnosis and treatment planning. While deep learning has excelled in\\nautomating this task, a major hurdle is the need for numerous annotated\\nsegmentation masks, which are resource-intensive to produce due to the required\\nexpertise and time. This scenario often leads to ultra low-data regimes, where\\nannotated images are extremely limited, posing significant challenges for the\\ngeneralization of conventional deep learning methods on test images. To address\\nthis, we introduce a generative deep learning framework, which uniquely\\ngenerates high-quality paired segmentation masks and medical images, serving as\\nauxiliary data for training robust models in data-scarce environments. Unlike\\ntraditional generative models that treat data generation and segmentation model\\ntraining as separate processes, our method employs multi-level optimization for\\nend-to-end data generation. This approach allows segmentation performance to\\ndirectly influence the data generation process, ensuring that the generated\\ndata is specifically tailored to enhance the performance of the segmentation\\nmodel. Our method demonstrated strong generalization performance across 9\\ndiverse medical image segmentation tasks and on 16 datasets, in ultra-low data\\nregimes, spanning various diseases, organs, and imaging modalities. When\\napplied to various segmentation models, it achieved performance improvements of\\n10-20\\% (absolute), in both same-domain and out-of-domain scenarios. Notably,\\nit requires 8 to 20 times less training data than existing methods to achieve\\ncomparable results. This advancement significantly improves the feasibility and\\ncost-effectiveness of applying deep learning in medical imaging, particularly\\nin scenarios with limited data availability.</td>\n",
              "      <td>[eess.IV, cs.CV]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99cbd294-a0ce-4a1d-9bd8-e9275c3a73ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99cbd294-a0ce-4a1d-9bd8-e9275c3a73ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99cbd294-a0ce-4a1d-9bd8-e9275c3a73ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1796addd-7d10-47ce-8cf2-caf0354bcbb7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1796addd-7d10-47ce-8cf2-caf0354bcbb7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1796addd-7d10-47ce-8cf2-caf0354bcbb7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  published  \\\n",
              "0 2024-08-30 17:55:05+00:00   \n",
              "1 2024-08-30 17:52:55+00:00   \n",
              "2 2024-08-30 17:35:06+00:00   \n",
              "3 2024-08-30 17:34:46+00:00   \n",
              "4 2024-08-30 17:29:25+00:00   \n",
              "5 2024-08-30 17:28:48+00:00   \n",
              "6 2024-08-30 17:26:46+00:00   \n",
              "7 2024-08-30 17:16:18+00:00   \n",
              "8 2024-08-30 17:12:14+00:00   \n",
              "9 2024-08-30 17:11:36+00:00   \n",
              "\n",
              "                                                                                                          title  \\\n",
              "0                                  The picasso gas model: Painting intracluster gas on gravity-only simulations   \n",
              "1                          Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding   \n",
              "2  DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model   \n",
              "3                                SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection   \n",
              "4                                             Advancing Multi-talker ASR Performance with Large Language Models   \n",
              "5                                            Virtual EVE: a Deep Learning Model for Solar Irradiance Prediction   \n",
              "6                        Neural network-based closure models for large-eddy simulations with explicit filtering   \n",
              "7                           CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion   \n",
              "8                                                       Open-vocabulary Temporal Action Localization using VLMs   \n",
              "9                                    Generative AI Enables Medical Image Segmentation in Ultra Low-Data Regimes   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 abstract  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                We introduce picasso, a model designed to predict thermodynamic properties of\\nthe intracluster medium based on the properties of halos in gravity-only\\nsimulations. The predictions result from the combination of an analytical gas\\nmodel, mapping gas properties to the gravitational potential, and of a machine\\nlearning model to predict the model parameters for individual halos based on\\ntheir scalar properties, such as mass and concentration. Once trained, the\\nmodel can be applied to make predictions for arbitrary potential distributions,\\nallowing its use with flexible inputs such as N-body particle distributions or\\nradial profiles. We present the model, and train it using pairs of gravity-only\\nand hydrodynamic simulations. We show that when trained on non-radiative\\nhydrodynamic simulations, picasso can make remarkably accurate and precise\\npredictions of intracluster gas thermodynamics. Training the model on\\nfull-physics simulations yields robust predictions as well, albeit with\\nslightly degraded performance. We further show that the model can be trained to\\nmake accurate predictions from very minimal information, at the cost of\\nmodestly reduced precision. picasso is made publicly available as a Python\\npackage, which includes trained models that can be used to make predictions\\neasily and efficiently, in a fully auto-differentiable and hardware-accelerated\\nframework.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           While existing research often treats long-form videos as extended short\\nvideos, we propose a novel approach that more accurately reflects human\\ncognition. This paper introduces BREASE: BRidging Episodes And SEmantics for\\nLong-Form Video Understanding, a model that simulates episodic memory\\naccumulation to capture action sequences and reinforces them with semantic\\nknowledge dispersed throughout the video. Our work makes two key contributions:\\nFirst, we develop an Episodic COmpressor (ECO) that efficiently aggregates\\ncrucial representations from micro to semi-macro levels. Second, we propose a\\nSemantics reTRiever (SeTR) that enhances these aggregated representations with\\nsemantic information by focusing on the broader context, dramatically reducing\\nfeature dimensionality while preserving relevant macro-level information.\\nExtensive experiments demonstrate that BREASE achieves state-of-the-art\\nperformance across multiple long video understanding benchmarks in both\\nzero-shot and fully-supervised settings. The project page and code are at:\\nhttps://joslefaure.github.io/assets/html/hermes.html.   \n",
              "2                                                                                                                                                                                                                                         Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\\nreconstruction and visualization. While foundation models like Depth Anything\\nModels (DAM) show promise, directly applying them to surgery often yields\\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\\noverfitting and catastrophic forgetting, compromising model robustness and\\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\\nissues, its uniform parameter distribution neglects the inherent feature\\nhierarchy, where earlier layers, learning more general features, require more\\nparameters than later ones. To tackle this issue, we introduce Depth Anything\\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\\nearlier layers and gradually decreasing parameters in later layers. We also\\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\\nperception by better tailoring the foundation model to the specific\\nrequirements of the surgical environment. The proposed method is validated on\\nthe SCARED dataset and demonstrates superior performance over recent\\nstate-of-the-art self-supervised monocular depth estimation techniques,\\nachieving an improvement of 13.3% in the absolute relative error metric. The\\ncode and pre-trained weights are available at\\nhttps://github.com/mobarakol/DARES.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Synthesizing the voices of unseen speakers is a persisting challenge in\\nmulti-speaker text-to-speech (TTS). Most multi-speaker TTS models rely on\\nmodeling speaker characteristics through speaker conditioning during training.\\nModeling unseen speaker attributes through this approach has necessitated an\\nincrease in model complexity, which makes it challenging to reproduce results\\nand improve upon them. We design a simple alternative to this. We propose\\nSelectTTS, a novel method to select the appropriate frames from the target\\nspeaker and decode using frame-level self-supervised learning (SSL) features.\\nWe show that this approach can effectively capture speaker characteristics for\\nunseen speakers, and achieves comparable results to other multi-speaker TTS\\nframeworks in both objective and subjective metrics. With SelectTTS, we show\\nthat frame selection from the target speaker's speech is a direct way to\\nachieve generalization in unseen speakers with low model complexity. We achieve\\nbetter speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E\\nwith over an 8x reduction in model parameters and a 270x reduction in training\\ndata   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Recognizing overlapping speech from multiple speakers in conversational\\nscenarios is one of the most challenging problem for automatic speech\\nrecognition (ASR). Serialized output training (SOT) is a classic method to\\naddress multi-talker ASR, with the idea of concatenating transcriptions from\\nmultiple speakers according to the emission times of their speech for training.\\nHowever, SOT-style transcriptions, derived from concatenating multiple related\\nutterances in a conversation, depend significantly on modeling long contexts.\\nTherefore, compared to traditional methods that primarily emphasize encoder\\nperformance in attention-based encoder-decoder (AED) architectures, a novel\\napproach utilizing large language models (LLMs) that leverages the capabilities\\nof pre-trained decoders may be better suited for such complex and challenging\\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\\nmulti-talker dataset using appropriate strategies. Experimental results\\ndemonstrate that our approach surpasses traditional AED-based methods on the\\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\\nevaluation set of the real-world dataset AMI, outperforming the AED model\\ntrained with 1000 times more supervised data in previous works.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                               Understanding space weather is vital for the protection of our terrestrial\\nand space infrastructure. In order to predict space weather accurately, large\\namounts of data are required, particularly in the extreme ultraviolet (EUV)\\nspectrum. An exquisite source of information for such data is provided by the\\nSolar Dynamic Observatory (SDO), which has been gathering solar measurements\\nfor the past 13 years. However, after a malfunction in 2014 affecting the\\nonboard Multiple EUV Grating Spectrograph A (MEGS-A) instrument, the scientific\\noutput in terms of EUV measurements has been significantly degraded. Building\\nupon existing research, we propose to utilize deep learning for the\\nvirtualization of the defective instrument. Our architecture features a linear\\ncomponent and a convolutional neural network (CNN) -- with EfficientNet as a\\nbackbone. The architecture utilizes as input grayscale images of the Sun at\\nmultiple frequencies -- provided by the Atmospheric Imaging Assembly (AIA) --\\nas well as solar magnetograms produced by the Helioseismic and Magnetic Imager\\n(HMI). Our findings highlight how AIA data are all that is needed for accurate\\npredictions of solar irradiance. Additionally, our model constitutes an\\nimprovement with respect to the state-of-the-art in the field, further\\npromoting the idea of deep learning as a viable option for the virtualization\\nof scientific instruments.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Data from direct numerical simulations of turbulent flows are commonly used\\nto train neural network-based models as subgrid closures for large-eddy\\nsimulations; however, models with low a priori accuracy have been observed to\\nfortuitously provide better a posteriori results than models with high a priori\\naccuracy. This anomaly can be traced to a dataset shift in the learning\\nproblem, arising from inconsistent filtering in the training and testing\\nstages. We propose a resolution to this issue that uses explicit filtering of\\nthe nonlinear advection term in the large-eddy simulation momentum equations to\\ncontrol aliasing errors. Within the context of explicitly-filtered large-eddy\\nsimulations, we develop neural network-based models for which a priori accuracy\\nis a good predictor of a posteriori performance. We evaluate the proposed\\nmethod in a large-eddy simulation of a turbulent flow in a plane channel at\\n$Re_{\\tau} = 180$. Our findings show that an explicitly-filtered large-eddy\\nsimulation with a filter-to-grid ratio of 2 sufficiently controls the numerical\\nerrors so as to allow for accurate and stable simulations.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         With advancements in video generative AI models (e.g., SORA), creators are\\nincreasingly using these techniques to enhance video previsualization. However,\\nthey face challenges with incomplete and mismatched AI workflows. Existing\\nmethods mainly rely on text descriptions and struggle with camera placement, a\\nkey component of previsualization. To address these issues, we introduce\\nCinePreGen, a visual previsualization system enhanced with engine-powered\\ndiffusion. It features a novel camera and storyboard interface that offers\\ndynamic control, from global to local camera adjustments. This is combined with\\na user-friendly AI rendering workflow, which aims to achieve consistent results\\nthrough multi-masked IP-Adapter and engine simulation guidelines. In our\\ncomprehensive evaluation study, we demonstrate that our system reduces\\ndevelopment viscosity (i.e., the complexity and challenges in the development\\nprocess), meets users' needs for extensive control and iteration in the design\\nprocess, and outperforms other AI video production workflows in cinematic\\ncamera movement, as shown by our experiments and a within-subjects user study.\\nWith its intuitive camera controls and realistic rendering of camera motion,\\nCinePreGen shows great potential for improving video production for both\\nindividual creators and industry professionals.   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Video action localization aims to find timings of a specific action from a\\nlong video. Although existing learning-based approaches have been successful,\\nthose require annotating videos that come with a considerable labor cost. This\\npaper proposes a learning-free, open-vocabulary approach based on emerging\\noff-the-shelf vision-language models (VLM). The challenge stems from the fact\\nthat VLMs are neither designed to process long videos nor tailored for finding\\nactions. We overcome these problems by extending an iterative visual prompting\\ntechnique. Specifically, we sample video frames into a concatenated image with\\nframe index labels, making a VLM guess a frame that is considered to be closest\\nto the start/end of the action. Iterating this process by narrowing a sampling\\ntime window results in finding a specific frame of start and end of an action.\\nWe demonstrate that this sampling technique yields reasonable results,\\nillustrating a practical extension of VLMs for understanding videos. A sample\\ncode is available at\\nhttps://microsoft.github.io/VLM-Video-Action-Localization/.   \n",
              "9  Semantic segmentation of medical images is pivotal in applications like\\ndisease diagnosis and treatment planning. While deep learning has excelled in\\nautomating this task, a major hurdle is the need for numerous annotated\\nsegmentation masks, which are resource-intensive to produce due to the required\\nexpertise and time. This scenario often leads to ultra low-data regimes, where\\nannotated images are extremely limited, posing significant challenges for the\\ngeneralization of conventional deep learning methods on test images. To address\\nthis, we introduce a generative deep learning framework, which uniquely\\ngenerates high-quality paired segmentation masks and medical images, serving as\\nauxiliary data for training robust models in data-scarce environments. Unlike\\ntraditional generative models that treat data generation and segmentation model\\ntraining as separate processes, our method employs multi-level optimization for\\nend-to-end data generation. This approach allows segmentation performance to\\ndirectly influence the data generation process, ensuring that the generated\\ndata is specifically tailored to enhance the performance of the segmentation\\nmodel. Our method demonstrated strong generalization performance across 9\\ndiverse medical image segmentation tasks and on 16 datasets, in ultra-low data\\nregimes, spanning various diseases, organs, and imaging modalities. When\\napplied to various segmentation models, it achieved performance improvements of\\n10-20\\% (absolute), in both same-domain and out-of-domain scenarios. Notably,\\nit requires 8 to 20 times less training data than existing methods to achieve\\ncomparable results. This advancement significantly improves the feasibility and\\ncost-effectiveness of applying deep learning in medical imaging, particularly\\nin scenarios with limited data availability.   \n",
              "\n",
              "                           categories  \n",
              "0                       [astro-ph.CO]  \n",
              "1               [cs.CV, cs.AI, cs.CL]  \n",
              "2                             [cs.CV]  \n",
              "3                    [eess.AS, cs.LG]  \n",
              "4                    [eess.AS, cs.AI]  \n",
              "5          [astro-ph.SR, astro-ph.IM]  \n",
              "6  [physics.flu-dyn, physics.comp-ph]  \n",
              "7                      [cs.CV, cs.HC]  \n",
              "8               [cs.CV, cs.AI, cs.RO]  \n",
              "9                    [eess.IV, cs.CV]  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Query to fetch AI-related papers\n",
        "query = 'ai OR artificial intelligence OR machine learning' #This tells arXiv to look for papers containing any of these terms.\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate) #Creates a Search Object\n",
        "# max_results=10 limits results to the 10 most recent\n",
        "# sort_by=arxiv.SortCriterion.SubmittedDate sort them by submission date\n",
        "\n",
        "# Fetch papers\n",
        "papers = []\n",
        "for result in search.results():\n",
        "    papers.append({\n",
        "      'published': result.published,\n",
        "        'title': result.title,\n",
        "        'abstract': result.summary,\n",
        "        'categories': result.categories\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None) #This tells pandas to not truncate long strings when displaying columns (like text in abstract or title).\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "bd1f9df427544ee394ed6bc541bb80e3",
            "19c804110f9d46ef93c122920501a76b",
            "d80accf23e714e1abdf05adba024346d",
            "6f0abfea6af3463f97855506438bdad9",
            "4d7f508afe4d41d9a8578b1bde82d896",
            "5afb0ae3e9c34ac9af74859ed3d596e4"
          ]
        },
        "id": "jYL4gtdWK7-W",
        "outputId": "e239f9b6-3905-4b2c-ac45-f9bd0968aaee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd1f9df427544ee394ed6bc541bb80e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19c804110f9d46ef93c122920501a76b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d80accf23e714e1abdf05adba024346d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f0abfea6af3463f97855506438bdad9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7f508afe4d41d9a8578b1bde82d896",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5afb0ae3e9c34ac9af74859ed3d596e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Example abstract from API\n",
        "abstract = df['abstract'][0]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4YqQRsGh20Gu",
        "outputId": "49209ec2-5362-453a-a98e-fa29f43c56e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' picasso is a model designed to predict thermodynamic properties of intracluster medium based on the properties of halos in gravity-onlysimulations. We show that when trained on non-radiative hydrodynamic simulations, picasso can make remarkably accurate and precise predictions of gas thermodynamics.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summarization_result[0]['summary_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bab3lu5lmrHk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}